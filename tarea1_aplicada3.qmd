---
title: "Estadística Aplicada 3 - Tarea 1"
lang: es
author: "Marcelino"
date: today
format:
  html:
    page-layout: full
    embed-resources: true
---


```{r, message=FALSE, warning=FALSE}
#Cargamos paquetes
library(tidymodels)
library(discrim)

library(corrr)
library(paletteer)

library(dslabs)
library(tidyr)

# Cargamos bases de datos
data2 <- iris
```

# 1.- Derive la probabilidad de mala clasificación para LDA

La probabilidad de mala clasificación binaria en LDA  (guassiano) es

$$P(\Delta)=P(\mathbf{x}\in R_{2}|\mathbf{x}\in \Pi_{1})\pi_{1}+P(\mathbf{x}\in R_{1}|\mathbf{x}\in \Pi_{2})\pi_{2}$$

donde tenemos que

$$P(\mathbf{x}\in R_{2}|\mathbf{x}\in \Pi_{1})\pi_{1} = P(L(\mathbf{x})<0|\mathbf{x}\in\Pi_{1})$$

Donde aquí consideramos que $L(\mathbf{x})$ es la función discriminante de LDA, $R_{2}$ es la región de decisión para la clase 2, y $\pi_{1}$ ,$\pi_{2}$ son las probabilidades a priori de las clases 1 y 2 respectivamente:

$$L(\mathbf{x})=\ln\left(\frac{f_{1}(x)\pi_{1}}{f_{2}(x)\pi_{2}}\right)= \ln\left(\frac{f_{1}(x)}{f_{2}(x)}\right)+ \ln\left(\frac{\pi_{1}}{\pi_{2}}\right)$$

Con lo cual desarrollando (usando propiedades de logaritmo) obtenemos la siguiente expresión:

$$L(\mathbf{x})=\ln(f_{1}(x))-\ln(f_{2}(x))+ \ln\left(\frac{\pi_{1}}{\pi_{2}}\right)=$$

$$\ln(f_{1}(x))-\ln(f_{2}(x))+ \ln\left(\frac{\pi_{1}}{\pi_{2}}\right)=$$

$$\ln \left(\frac{1}{\sqrt{(2\pi)^{k} |\Sigma|}} \exp\left(-\frac{1}{2} (\mathbf{x} - \mu_{1})^T \Sigma^{-1} (\mathbf{x} - \mu_{1})\right)\right)$$

$$-\ln \left( \frac{1}{\sqrt{(2\pi)^k |\Sigma|}} \exp\left(-\frac{1}{2} (\mathbf{x} - \mu_{2})^T \Sigma^{-1} (\mathbf{x} - \mu_{2})\right)\right)+ \ln\left(\frac{\pi_{1}}{\pi_{2}}\right)=$$

$$\left(-\frac{1}{2} (\mathbf{x} - \mu_{1})^T \Sigma^{-1} (\mathbf{x} - \mu_{1})\right)-$$

$$\left(-\frac{1}{2} (\mathbf{x} - \mu_{2})^T \Sigma^{-1} (\mathbf{x} - \mu_{2})\right)+ \ln\left(\frac{\pi_{1}}{\pi_{2}}\right)=$$

A parti de aquí aplicamos las propiedades de simetría de $\Sigma$ y además con el desarrollo algebraico obtenemos: 

$$L(\mathbf{x})=\left(-\frac{1}{2}\left(\mu_{1}^T \Sigma^{-1} \mu_{1}-\mu_{2}^T \Sigma^{-1} \mu_{2}\right)+ \ln\left(\frac{\pi_{1}}{\pi_{2}}\right)\right)+$$

$$\mu_{1}^T \Sigma^{-1}\mathbf{x}- \mu_{2}^T \Sigma^{-1}\mathbf{x}$$

Con lo cual podemos reescribir la expresión como:

$$L(\mathbf{x})= b_{0} + \mathbf{b}'\mathbf{x}$$

donde 

$$b_{0}=-\frac{1}{2}\left(\mu_{1}^T \Sigma^{-1} \mu_{1}-\mu_{2}^T \Sigma^{-1} \mu_{2}\right)+ \ln\left(\frac{\pi_{1}}{\pi_{2}}\right)$$

$$\mathbf{b}=\Sigma^{-1}(\mu_{1}-\mu_{2})$$

Ahora volviendo a lo que nos concierne, considerando la probabilidad condicional, tendríamos la siguiente igualdad:

$$P(L(\mathbf{x})<0|\mathbf{x}\in\Pi_{1})=P_{\Pi_{1}}(b_{0} + \mathbf{b}'\mathbf{x}<0)=$$

Ahora estandarizamos la expresión:

$$P_{\Pi_{1}}\left(\frac{\mathbf{b}'\mathbf{x}-\mathbf{b}'\mu_{1}}{\sqrt{(b'\Sigma b)}}<-\frac{(b_{0}+ \mathbf{b}'\mu_{1})}{\sqrt{b'\Sigma b}}\right)=\Phi\left(-\frac{(b_{0}+ \mathbf{b}'\mu_{1})}{\sqrt{b'\Sigma b}}\right)$$

y análogamente para la otra probabilidad que tendríamos que calcular tendríamos:

$$P(\mathbf{x}\in R_{1}|\mathbf{x}\in \Pi_{2})=\Phi\left(\frac{(b_{0}+ \mathbf{b}'\mu_{2})}{\sqrt{b'\Sigma b}}\right)$$


Con lo cual la probabilidad de error de clasificación es el siguiente:

$$P(\Delta)=\Phi\left(-\frac{(b_{0}+ \mathbf{b}'\mu_{1})}{\sqrt{b'\Sigma b}}\right)\pi_{1}+\Phi\left(\frac{(b_{0}+ \mathbf{b}'\mu_{2})}{\sqrt{b'\Sigma b}}\right)\pi_{2}$$

y haciendo una mejor simplificación tendríamos:

$$P(\Delta)=\Phi\left(-\frac{(b_{0}+ \mathbf{b}'\mu_{1})}{\sqrt{b'\Sigma b}}\right)\pi_{1}+\Phi\left(\frac{(b_{0}+ \mathbf{b}'\mu_{2})}{\sqrt{b'\Sigma b}}\right)\pi_{2}$$

2. **Implementar LDA sobre la base de datos ```MNIST``` y usar los dígitos 1 y 3**

La implementación en R fue usando la librería de ```MASS``` para el método de LDA, y la librería ```dslabs``` para acceder a los datos de MNIST

Se inicializan los datos de la siguiente forma:

```
mnist_data <- read_mnist()
```

Los datos ya vienen divididos en variables ```train``` y ```test```, las que a su vez contienen ```images``` y ```labels```.

Ahora, los datos de imágenes 28x28 vienen en forma de vectores de 784 entradas, con valores del 0 al 255 representando la gamma del gris. Debido a su fuerte correlación entre pixeles adyacentes, es importante reducir la colinearidad al tomar en vez un estadístico de los pixeles que nos ayude todavía a diferenciarlos. Para esto, decidimos tomar el promedio de los renglones de la imagen para determinar cuánto está dibujado de negro cada fila en la imagen; un dato que debe ayudarnos en general a distinguir 1s de 3s.

```
#Extraer el train y test
tri <- train_images <- mnist_data$train$images
train_labels<-mnist_data$train$labels
tei <- test_images <- mnist_data$test$images
test_labels <- mnist_data$test$labels

#Transformar el train al estadístico que deseamos de solo 1 y 3
tr_ind <- (train_labels == 1) | (train_labels==3)
train_labels <- train_labels[tr_ind]
tri <- tri[tr_ind,]
tri2 <- array(tri, dim=c(dim(tri)[1],28,28))
tri_promr <- rowMeans(tri2, dims = 2)

#Transformar el test al estadístico que deseamos de solo 1 y 3
te_ind <- (test_labels == 1) | (test_labels==3)
test_labels <- test_labels[te_ind]
tei <- tei[te_ind,]
tei2 <- array(tei, dim=c(dim(tei)[1],28,28))
tei_promr <- rowMeans(tei2, dims = 2)
```

Una vez hecho eso, ahora aplicamos el algoritmo de LDA para crear el predictor, construyendo la matriz (Y,X) para insertar al modelo.

```
train <- data.frame(label = (train_labels == 1),tri_promr)

model <- lda(label~.,data=train)
```

Una vez entrenado, ahora lo ponemos a prueba con el set de ```test```, y medimos la precisión.

```
test <- data.frame(label = (test_labels == 1),tei_promr)


prob <- predict(model, test, method = "debiased") #, type = "response")
pred <- ifelse(prob$posterior[,2] > 0.5, 1, 3)

sum(pred == test_labels) / length(test_labels) #precisión de predictor

```

Con ello, obtenemos al final los resultados del experimento, lo cual nos da:

```
> prob <- predict(model, test, method = "debiased") #, type = "response")
> pred <- ifelse(prob$posterior[,2] > 0.5, 1, 3)
> sum(pred == test_labels) / length(test_labels) #precisión de predictor
[1] 0.8899767
```

Eso nos da una precisión del 88.99%, bastante alto para un predictor lineal de clases entre 1 y 3. Con ello, se conluye el problema.

# 3.- Implementar QDA sobre los datos de ```iris``` (raw data)


```{r, error=TRUE}
qda_spec <- discrim_quad() |>
  set_mode("classification") |>
  set_engine("MASS")

qda_fit <- qda_spec |>
  fit(Species ~ ., data = data2) #modificar

augment(qda_fit, new_data = data2) |>
  conf_mat(truth = Species, estimate = .pred_class) |>
  autoplot(type = "heatmap")

augment(qda_fit, new_data = data2) |>
  accuracy(truth = Species, estimate = .pred_class)

augment(qda_fit, new_data = data2) |>
  recall(truth = Species, estimate = .pred_class)

augment(qda_fit, new_data = data2) |>
  precision(truth = Species, estimate = .pred_class)

qda_fit |>
  pluck("fit")
```

# 5.- Problema de clasificación

A reseracher wants to determine a prcedure for disminating between two multivariate populations. The researcher has enough datavailable to estimate the density functions $f_{1}(x)$ and $f_{2}(x)$ associated with populatios $\pi_{1}$ and $\pi_{2}$ respectively. Let $c(2|1)=50$ (cost of assigning item as $\pi_{2}$ given that $\pi_{1} is true$) and $c(2|1)=100$

In addition, it is knoe that about 20% of all possible items (for which the measurements $x$ can be recorded) belong to $\pi_{2}$ 

(a) Give the minimum ECM rule (in general form) for assigning a new item to one of the two populations.



(b) Measurements recorded on a new item yield the densitiy values $f_{1}(x)=.3$ and $f_{2}(x)=.5$. Given the preceding information, assign this item to populatio $\pi_{1}$ or $\pi_{2}$


# 6.- Problema de QDA

Suppose $x$ comes from one of two populations:

$$\pi_{1}\sim N(\mu_{1}, \Sigma_{1})$$

$$\pi_{2}\sim N(\mu_{2}, \Sigma_{2})$$

If the respective density functions are denoted by f_{1}(x) and f_{2}(x), find the expression for the quadratic disccriminator

$$Q=\ln\left(\frac{f_{1}(x)}{f_{2}(x)}\right)$$

If $\Sigma_{1}=\Sigma_{2}=\Sigma$ for instance, verify that $Q$ becomes 

$$(\mu_{1}-\mu_{2})'\Sigma^{-1}x-\frac{1}{2}(\mu_{1}-\mu_{2})'\Sigma^{-1}(\mu_{1}+\mu_{2})$$


# 7.- Write a coputer program to implement single-linkage, average-linkage, and complete-linkage agglomerative hierarchical clustering. Try it out on a data set of your choice.

```{r, error=TRUE}

# Sample data
set.seed(123)
data <- matrix(rnorm(30), ncol=2)
colnames(data) <- c("X", "Y")

# Visualize the data
plot(data, pch=19, col="blue", xlab="X", ylab="Y", main="Sample Data Points")

dist_matrix <- dist(data)

clusterFunc <- function(dist_matrix,method ="single" ){
  if(method=="single"){
    # Single-linkage
    single_linkage <- hclust(dist_matrix, method="single")
    plot(single_linkage, main="Single-linkage Hierarchical Clustering", sub="", xlab="", ylab="Height")
  } else if(method=="average"){
    # Average-linkage
    average_linkage <- hclust(dist_matrix, method="average")
    plot(average_linkage, main="Average-linkage Hierarchical Clustering", sub="", xlab="", ylab="Height")
  } else{
    # Complete-linkage
    complete_linkage <- hclust(dist_matrix, method="complete")
    plot(complete_linkage, main="Complete-linkage Hierarchical Clustering", sub="", xlab="", ylab="Height")
  }
}

clusterFunc(dist_matrix,method ="single" )
clusterFunc(dist_matrix,method ="average" )
clusterFunc(dist_matrix,method ="complete" )

```

# 8.- Implemente SL, AL y CL sobre la base de datos ```iris``` (raw data)

```{r}


```



# 9.- Problem 12.13

The following table lists measurements on 5 nutritional variables for 12 breakfast cereals.

```{r, message=FALSE, warning=FALSE}
cereal_data <- data.frame(
  Cereal = c("Life", "Grape Nuts", "Super Sugar Crisp", "Special K", "Rice Krispies", "Raisin Bran", 
             "Product 19", "Wheaties", "Total", "Puffed Rice", "Sugar Corn Pops", "Sugar Smacks"),
  Protein_gm = c(6, 3, 2, 6, 2, 3, 2, 3, 3, 1, 1, 2),
  Carbohydrates_gm = c(19, 23, 26, 21, 25, 28, 24, 23, 23, 13, 26, 25),
  Fat_gm = c(1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0),
  Calories_per_oz = c(110, 100, 110, 110, 110, 120, 110, 110, 110, 50, 110, 110),
  Vitamin_A_pct_daily_allowance = c(0, 25, 25, 25, 25, 25, 100, 25, 100, 0, 25, 25)
)

cereal_data
```

(a) Using tha data in the table, calculate the euclidean distances between each pair of cereal brands.

```{r}
# Compute the distance matrix
dist_matrix <- dist(cereal_data[1:12,2:6])

dist_matrix
```

(b) Treating the distances calculated in (a) as measures of dissimilarity, use single-linkage, complete-linkage, and average-linkage clustering to cluster the cereal brands. Construct a dendrogram for each method.

```{r}
clusterFunc(dist_matrix,method ="single" )
clusterFunc(dist_matrix,method ="average" )
clusterFunc(dist_matrix,method ="complete" )
```


# 10.- Input the data in the previous table into k-means clustering program. Use the program to cluster the cereal brands into 2, 3, and 4 clusters. Compare the results with those obtained in the previous problem.

## Utilizamos K=2
```{r}
# Exclude the Cereal column
clustering_data <- cereal_data[,-1]

# Choose the optimal number of clusters (for demonstration, let's say 3)
set.seed(123)  # Set seed for reproducibility
clusters <- kmeans(clustering_data, centers=2)

# Add cluster results to the original data
cereal_data$cluster <- as.factor(clusters$cluster)

# Print the dataframe with clusters
print(cereal_data)
```

## Utilizamos K=3
```{r}
clusters <- kmeans(clustering_data, centers=3)

# Add cluster results to the original data
cereal_data$cluster <- as.factor(clusters$cluster)

# Print the dataframe with clusters
print(cereal_data)
```

## Utilizamos K=4
```{r}
clusters <- kmeans(clustering_data, centers=4)

# Add cluster results to the original data
cereal_data$cluster <- as.factor(clusters$cluster)

# Print the dataframe with clusters
print(cereal_data)
```

**Poner interpretación**

