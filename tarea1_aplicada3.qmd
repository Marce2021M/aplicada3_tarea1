---
title: "Estadística Aplicada 3 - Tarea 1"
lang: es
author: "Marcelino"
date: today
format:
  html:
    page-layout: full
    embed-resources: true
---


```{r, message=FALSE, warning=FALSE}
#Cargamos paquetes
library(tidymodels)
library(discrim)

library(corrr)
library(paletteer)

library(dslabs)
library(tidyr)

# Cargamos bases de datos
data2 <- iris
```

# 1.- Derive la probabilidad de mala clasificación para LDA

La probabilidad de mala clasificación binaria en LDA  (guassiano) es

$$P(\Delta)=P(\mathbf{x}\in R_{2}|\mathbf{x}\in \Pi_{1})\pi_{1}+P(\mathbf{x}\in R_{1}|\mathbf{x}\in \Pi_{2})\pi_{2}$$

donde tenemos que

$$P(\mathbf{x}\in R_{2}|\mathbf{x}\in \Pi_{1})\pi_{1} = P(L(\mathbf{x})<0|\mathbf{x}\in\Pi_{1})$$

donde aquí consideramos que $L(\mathbf{x})$ es la función discriminante de LDA, y $R_{2}$ es la región de decisión para la clase 2:

$$L(\mathbf{x})=\ln\left(\frac{f_{1}(x)\pi_{1}}{f_{2}(x)\pi_{2}}\right)= \ln\left(\frac{f_{1}(x)}{f_{2}(x)}\right)+ \ln\left(\frac{\pi_{1}}{\pi_{2}}\right)$$

Con lo cual desarrollando (usando propiedades de logaritmo) obtenemos la siguiente expresión:

$$L(\mathbf{x})=\ln(f_{1}(x))-\ln(f_{2}(x))+ \ln\left(\frac{\pi_{1}}{\pi_{2}}\right)=$$

$$\ln(f_{1}(x))-\ln(f_{2}(x))+ \ln\left(\frac{\pi_{1}}{\pi_{2}}\right)=$$

$$\ln \left(\frac{1}{\sqrt{(2\pi)^k |\Sigma|}} \exp\left(-\frac{1}{2} (\mathbf{x} - \mu_{1})^T \Sigma^{-1} (\mathbf{x} - \mu_{1})\right)
\right)-$$

$$\ln \left( \frac{1}{\sqrt{(2\pi)^k |\Sigma|}} \exp\left(-\frac{1}{2} (\mathbf{x} - \mu_{2})^T \Sigma^{-1} (\mathbf{x} - \mu_{2})\right)
\right)+ \ln\left(\frac{\pi_{1}}{\pi_{2}}\right)=$$

$$\left(-\frac{1}{2} (\mathbf{x} - \mu_{1})^T \Sigma^{-1} (\mathbf{x} - \mu_{1})\right)
-$$

$$\left(-\frac{1}{2} (\mathbf{x} - \mu_{2})^T \Sigma^{-1} (\mathbf{x} - \mu_{2})\right)
+ \ln\left(\frac{\pi_{1}}{\pi_{2}}\right)=$$



# 2.- Implementar LDA sobre la base de datos ```MNIST``` y usar los dígitos 1 y 3

```{r, error=TRUE, eval=FALSE}
#Cargamos bases de datos
mnist_data <- read_mnist()

#Preparamos datos
tri <- train_images <- mnist_data$train$images
train_labels<-mnist_data$train$labels
tei <- test_images <- mnist_data$test$images
test_labels <- mnist_data$test$labels

## Entrenamiento
tr_ind <- (train_labels == 1) | (train_labels==3)
train_labels <- train_labels[tr_ind]
tri <- tri[tr_ind,]
tri2 <- array(tri, dim=c(dim(tri)[1],28,28))
tri_promr <- rowMeans(tri2, dims = 2)

#Testeo
te_ind <- (test_labels == 1) | (test_labels==3)
test_labels <- test_labels[te_ind]
tei <- tei[te_ind,]
tei2 <- array(tei, dim=c(dim(tei)[1],28,28))
tei_promr <- rowMeans(tei2, dims = 2)

#Hacemos dataframe para entrenar
train <- data.frame(label = (train_labels == 1),tri_promr) 
train$label <- as.factor(train$label) #agregamos

# modelo LDA

lda_spec <- discrim_linear() |>
  set_mode("classification") |>
  set_engine("MASS")

train$label <- as.factor(train$label)

lda_fit <- lda_spec |>
  fit(label ~ ., data = train) #modificar



#model <- lda(label~.,data=train)

#Hacemos dataframe para testear

test <- data.frame(label = (test_labels == 1),tei_promr)

prob <- predict(model, test, method = "debiased") #, type = "response")
pred <- ifelse(prob$posterior[,2] > 0.5, 1, 3)

sum(pred == test_labels) / length(test_labels) #precisión de predictor

test$label <- as.factor(test$label) #agregamos

augment(lda_fit, new_data = test) |>
  conf_mat(truth = Species, estimate = .pred_class) |>
  autoplot(type = "heatmap")

augment(lda_fit, new_data = test) |>
  accuracy(truth = Species, estimate = .pred_class)

augment(lda_fit, new_data = test) |>
  recall(truth = Species, estimate = .pred_class)

augment(lda_fit, new_data = test) |>
  precision(truth = Species, estimate = .pred_class)

qda_fit |>
  pluck("fit")

```

```{r, eval=FALSE}
# cv

#data
mnist_data <- read_mnist()

tri <- train_images <- mnist_data$train$images
train_labels<-mnist_data$train$labels
tei <- test_images <- mnist_data$test$images
test_labels <- mnist_data$test$labels

tr_ind <- (train_labels == 1) | (train_labels==3)
train_labels <- train_labels[tr_ind]
tri <- tri[tr_ind,]

train <- data.frame(label = (train_labels == 1),tri)
train$label <- as.factor(train$label)
# All operating systems
library(doParallel)

# Create a cluster object and then register: 
cl <- makePSOCKcluster(6)
registerDoParallel(cl)

options <- control_resamples(verbose = TRUE)

# Create glm specification model

lr_spec <- logistic_reg() |>  
  set_engine("glm") 

lr_recipe <- recipe(label ~ ., 
    data = train) 

lr_workflow <- workflow(lr_recipe, lr_spec)

# Create folds
train_folds <- vfold_cv(train, v = 3, repeats = 5)
# Fit the model
system.time({
lr_fit_cv <- lr_workflow |> 
  fit_resamples(train_folds, control = options)
})


```

# 3.- Implementar QDA sobre los datos de ```iris``` (raw data)


```{r, error=TRUE}
qda_spec <- discrim_quad() |>
  set_mode("classification") |>
  set_engine("MASS")

qda_fit <- qda_spec |>
  fit(Species ~ ., data = data2) #modificar

augment(qda_fit, new_data = data2) |>
  conf_mat(truth = Species, estimate = .pred_class) |>
  autoplot(type = "heatmap")

augment(qda_fit, new_data = data2) |>
  accuracy(truth = Species, estimate = .pred_class)

augment(qda_fit, new_data = data2) |>
  recall(truth = Species, estimate = .pred_class)

augment(qda_fit, new_data = data2) |>
  precision(truth = Species, estimate = .pred_class)

qda_fit |>
  pluck("fit")
```

# 5.- Problema de clasificación

A reseracher wants to determine a prcedure for disminating between two multivariate populations. The researcher has enough datavailable to estimate the density functions $f_{1}(x)$ and $f_{2}(x)$ associated with populatios $\pi_{1}$ and $\pi_{2}$ respectively. Let $c(2|1)=50$ (cost of assigning item as $\pi_{2}$ given that $\pi_{1} is true$) and $c(2|1)=100$

In addition, it is knoe that about 20% of all possible items (for which the measurements $x$ can be recorded) belong to $\pi_{2}$ 

(a) Give the minimum ECM rule (in general form) for assigning a new item to one of the two populations.



(b) Measurements recorded on a new item yield the densitiy values $f_{1}(x)=.3$ and $f_{2}(x)=.5$. Given the preceding information, assign this item to populatio $\pi_{1}$ or $\pi_{2}$


# 6.- Problema de QDA

Suppose $x$ comes from one of two populations:

$$\pi_{1}\sim N(\mu_{1}, \Sigma_{1})$$

$$\pi_{2}\sim N(\mu_{2}, \Sigma_{2})$$

If the respective density functions are denoted by f_{1}(x) and f_{2}(x), find the expression for the quadratic disccriminator

$$Q=\ln\left(\frac{f_{1}(x)}{f_{2}(x)}\right)$$

If $\Sigma_{1}=\Sigma_{2}=\Sigma$ for instance, verify that $Q$ becomes 

$$(\mu_{1}-\mu_{2})'\Sigma^{-1}x-\frac{1}{2}(\mu_{1}-\mu_{2})'\Sigma^{-1}(\mu_{1}+\mu_{2})$$


# 7.- Write a coputer program to implement single-linkage, average-linkage, and complete-linkage agglomerative hierarchical clustering. Try it out on a data set of your choice.

```{r, error=TRUE}

# Sample data
set.seed(123)
data <- matrix(rnorm(30), ncol=2)
colnames(data) <- c("X", "Y")

# Visualize the data
plot(data, pch=19, col="blue", xlab="X", ylab="Y", main="Sample Data Points")

dist_matrix <- dist(data)

clusterFunc <- function(dist_matrix,method ="single" ){
  if(method=="single"){
    # Single-linkage
    single_linkage <- hclust(dist_matrix, method="single")
    plot(single_linkage, main="Single-linkage Hierarchical Clustering", sub="", xlab="", ylab="Height")
  } else if(method=="average"){
    # Average-linkage
    average_linkage <- hclust(dist_matrix, method="average")
    plot(average_linkage, main="Average-linkage Hierarchical Clustering", sub="", xlab="", ylab="Height")
  } else{
    # Complete-linkage
    complete_linkage <- hclust(dist_matrix, method="complete")
    plot(complete_linkage, main="Complete-linkage Hierarchical Clustering", sub="", xlab="", ylab="Height")
  }
}

clusterFunc(dist_matrix,method ="single" )
clusterFunc(dist_matrix,method ="average" )
clusterFunc(dist_matrix,method ="complete" )

```

# 8.- Implemente SL, AL y CL sobre la base de datos ```iris``` (raw data)

```{r}


```



# 9.- Problem 12.13

The following table lists measurements on 5 nutritional variables for 12 breakfast cereals.

```{r, message=FALSE, warning=FALSE}
cereal_data <- data.frame(
  Cereal = c("Life", "Grape Nuts", "Super Sugar Crisp", "Special K", "Rice Krispies", "Raisin Bran", 
             "Product 19", "Wheaties", "Total", "Puffed Rice", "Sugar Corn Pops", "Sugar Smacks"),
  Protein_gm = c(6, 3, 2, 6, 2, 3, 2, 3, 3, 1, 1, 2),
  Carbohydrates_gm = c(19, 23, 26, 21, 25, 28, 24, 23, 23, 13, 26, 25),
  Fat_gm = c(1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0),
  Calories_per_oz = c(110, 100, 110, 110, 110, 120, 110, 110, 110, 50, 110, 110),
  Vitamin_A_pct_daily_allowance = c(0, 25, 25, 25, 25, 25, 100, 25, 100, 0, 25, 25)
)

cereal_data
```

(a) Using tha data in the table, calculate the euclidean distances between each pair of cereal brands.

```{r}
# Compute the distance matrix
dist_matrix <- dist(cereal_data[1:12,2:6])

dist_matrix
```

(b) Treating the distances calculated in (a) as measures of dissimilarity, use single-linkage, complete-linkage, and average-linkage clustering to cluster the cereal brands. Construct a dendrogram for each method.

```{r}
clusterFunc(dist_matrix,method ="single" )
clusterFunc(dist_matrix,method ="average" )
clusterFunc(dist_matrix,method ="complete" )
```


# 10.- Input the data in the previous table into k-means clustering program. Use the program to cluster the cereal brands into 2, 3, and 4 clusters. Compare the results with those obtained in the previous problem.

## Utilizamos K=2
```{r}
# Exclude the Cereal column
clustering_data <- cereal_data[,-1]

# Choose the optimal number of clusters (for demonstration, let's say 3)
set.seed(123)  # Set seed for reproducibility
clusters <- kmeans(clustering_data, centers=2)

# Add cluster results to the original data
cereal_data$cluster <- as.factor(clusters$cluster)

# Print the dataframe with clusters
print(cereal_data)
```

## Utilizamos K=3
```{r}
clusters <- kmeans(clustering_data, centers=3)

# Add cluster results to the original data
cereal_data$cluster <- as.factor(clusters$cluster)

# Print the dataframe with clusters
print(cereal_data)
```

## Utilizamos K=4
```{r}
clusters <- kmeans(clustering_data, centers=4)

# Add cluster results to the original data
cereal_data$cluster <- as.factor(clusters$cluster)

# Print the dataframe with clusters
print(cereal_data)
```

**Poner interpretación**

